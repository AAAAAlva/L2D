{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 0 -- Gathering Relevant Articles in Full-Text Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 -- I. Retrieving the DOI List of Articles via Elsevier API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "\n",
    " Your ScienceDirect API key ()\n",
    "elsevier_api_key = 'xxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "# Define the API endpoint and query parameters\n",
    "url = 'https://api.elsevier.com/content/search/sciencedirect'\n",
    "query_params = {\n",
    "    'query': 'anaerobic biodegradation',\n",
    "    'count': 10,  # Retrieve 10 articles\n",
    "    'start': 0   # Start at the 1st result\n",
    "}\n",
    "\n",
    "# Add the API key to the request headers\n",
    "headers = {\n",
    "    'X-ELS-APIKey': elsevier_api_key,\n",
    "    'Accept': 'application/json'\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(url, params=query_params, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response as JSON\n",
    "    search_results = response.json()\n",
    "    articles = search_results['search-results']['entry']\n",
    "    \n",
    "    # Initialize an empty list to store DOIs\n",
    "    doi_list = []\n",
    "    \n",
    "    # Extract the DOIs from the search results\n",
    "    for article in articles:\n",
    "        if 'prism:doi' in article:  # Ensure the article has a DOI\n",
    "            doi_list.append(article['prism:doi'])\n",
    "    \n",
    "    # Save the DOIs to a CSV file\n",
    "    with open('DOI_list.csv', 'w', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow(['DOI'])  # Write header\n",
    "        for doi in doi_list:\n",
    "            csv_writer.writerow([doi])\n",
    "    \n",
    "    print(f\"Retrieved {len(doi_list)} DOIs and saved them to DOI_list.csv.\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 -- II. Using the DOI List to Download Articles (.xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the full-text from DOI as XML file for multiple DOIs in a list\n",
    "\n",
    "import httpx\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Function to get full-text XML for a given DOI\n",
    "def get_full_text_by_doi(paper_doi, apikey):\n",
    "    headers = {\n",
    "        \"X-ELS-APIKey\": apikey,\n",
    "        \"Accept\": 'application/xml'  # Request XML format\n",
    "    }\n",
    "    \n",
    "    # Set timeout for requests\n",
    "    #timeout = httpx.Timeout(10.0, connect=60.0)\n",
    "    timeout = httpx.Timeout(60.0, connect=60.0)  # Wait up to 60 seconds for a response\n",
    "    client = httpx.Client(timeout=timeout, headers=headers)\n",
    "    \n",
    "    # Elsevier API endpoint for retrieving full text by DOI\n",
    "    url = f\"https://api.elsevier.com/content/article/doi/{paper_doi}\"\n",
    "    \n",
    "    # Make the request\n",
    "    response = client.get(url)\n",
    "    \n",
    "    print(f\"Request for DOI {paper_doi}: {response.status_code}\")  # Print the response status\n",
    "    return response\n",
    "\n",
    "# Function to save XML content to file using order number for filename\n",
    "def save_xml_content(xml_content, order_num):\n",
    "    # Create filename using order number (e.g., 1_full_text.xml)\n",
    "    filename = f\"{order_num}_full_text.xml\"\n",
    "    \n",
    "    # Save the XML content to a file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(xml_content)\n",
    "    print(f\"Saved XML for Order Number {order_num} to {filename}\")\n",
    "\n",
    "# Your ScienceDirect API key\n",
    "elsevier_api_key = 'xxxxxxxxxxxxxxxxxxxx'\n",
    "\n",
    "# Path to the input CSV file containing DOIs\n",
    "input_csv_file_path = 'DOI_list.csv'\n",
    "\n",
    "# Path to the output CSV file where we'll save the order numbers and DOIs\n",
    "output_csv_file_path = 'ordered_doi_list.csv'\n",
    "\n",
    "# Read the list of DOIs from the input CSV file\n",
    "with open(input_csv_file_path, newline='') as csvfile:\n",
    "    csv_reader = csv.reader(csvfile)\n",
    "    next(csv_reader)  # Skip the header\n",
    "    doi_list = [row[0] for row in csv_reader]  # Extract DOIs from the first column\n",
    "\n",
    "# Write the new CSV file with order numbers and DOIs\n",
    "with open(output_csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    csv_writer.writerow(['Order Number', 'DOI'])  # Write the header\n",
    "\n",
    "    # Loop through the list of DOIs and assign an order number\n",
    "    for i, doi in enumerate(doi_list, start=1):  # Order numbers starting from 1\n",
    "        csv_writer.writerow([i, doi])  # Write the order number and DOI to the new CSV file\n",
    "        \n",
    "        # Get the full-text XML for the current DOI\n",
    "        response = get_full_text_by_doi(doi, elsevier_api_key)\n",
    "        \n",
    "        # Check if the response was successful\n",
    "        if response.status_code == 200:\n",
    "            xml_content = response.text\n",
    "            \n",
    "            # Save the XML content to a file named as \"<order_number>_full_text.xml\"\n",
    "            save_xml_content(xml_content, i)\n",
    "        else:\n",
    "            print(f\"Failed to retrieve XML for DOI {doi}, Status code: {response.status_code}\")\n",
    "        \n",
    "        # Optional: Add a delay between requests to avoid hitting rate limits\n",
    "        time.sleep(2)  # Adjust as needed (e.g., 2 seconds between requests)\n",
    "\n",
    "print(f\"Process completed. DOIs and order numbers saved to {output_csv_file_path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 -- III. Cleaning the XML Files and Converting to Text (.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "\n",
    "# Helper function to clean unnecessary tags and return paragraph text\n",
    "def get_cleaned_paragraph(para_element):\n",
    "    para_text = \"\"\n",
    "    # Iterate through sub-elements, including the text and any tail text\n",
    "    for sub_elem in para_element.iter():\n",
    "        # Ignore unnecessary tags but collect text and tail text\n",
    "        tags_to_ignore = ['ce:inf', 'ce:sup', 'ce:formula', 'ce:display', 'ce:italic', 'ce:chem', 'ce:float-anchor', 'mml:mo', 'mml:mi', 'mml:mfenced', 'mml:msub', 'mml:msup', 'ce:bold', 'ce:hsp', 'mml:math', 'ce:cross-refs', 'ce:cross-ref', 'mml:mrow', 'mml:msubsup']\n",
    "        if sub_elem.tag not in tags_to_ignore:\n",
    "            if sub_elem.text:\n",
    "                para_text += sub_elem.text.strip() + \" \"\n",
    "            if sub_elem.tail:\n",
    "                para_text += sub_elem.tail.strip() + \" \"\n",
    "    return para_text.strip()\n",
    "\n",
    "# Function to process an XML file and extract clean text\n",
    "def process_xml_file(input_filename, output_filename):\n",
    "    # Read the XML file\n",
    "    with open(input_filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        xml_content = file.read()\n",
    "\n",
    "    # Parse the XML content\n",
    "    root = ET.fromstring(xml_content)\n",
    "\n",
    "    # Initialize variables to store extracted titles and paragraphs\n",
    "    extracted_content = []\n",
    "\n",
    "    # Iterate over the XML elements and extract titles and paragraphs\n",
    "    for elem in root.iter():\n",
    "        # Extract section titles\n",
    "        if elem.tag.endswith('section-title') and elem.text:\n",
    "            extracted_content.append(f\"Title: {elem.text.strip()}\")\n",
    "        \n",
    "        # Extract paragraphs\n",
    "        if elem.tag.endswith('para') and elem.attrib.get('view') == 'all':\n",
    "            cleaned_para = get_cleaned_paragraph(elem)\n",
    "            if cleaned_para:  # Only add if there's actual text\n",
    "                extracted_content.append(f\"Paragraph: {cleaned_para}\")\n",
    "\n",
    "    # Combine all extracted content into a single text\n",
    "    cleaned_text = \"\\n\\n\".join(extracted_content)\n",
    "\n",
    "    # Save the cleaned text to a file\n",
    "    with open(output_filename, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(cleaned_text)\n",
    "\n",
    "    print(f\"Processed and saved: {output_filename}\")\n",
    "\n",
    "# Main loop to process multiple XML files\n",
    "for i in range(1, 111):  # Loop through files 1 to 10\n",
    "    input_filename = f\"{i}_full_text.xml\"  # Input file name (e.g., 1_full_text.xml)\n",
    "    output_filename = f\"{i}_clean_text.txt\"  # Output file name (e.g., 1_clean_text.txt)\n",
    "\n",
    "    # Check if the input file exists before processing\n",
    "    if os.path.exists(input_filename):\n",
    "        process_xml_file(input_filename, output_filename)\n",
    "    else:\n",
    "        print(f\"File {input_filename} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 0 -- IV. Converting User-Provided PDF Files to Cleaned Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader('1_DOI.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1 -- Screening Relevant Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"XXXXXXXXXXXXX\",  # Replace with your actual OpenAI API key\n",
    ")\n",
    "\n",
    "def chat_gpt(clean_text):\n",
    "    # Define the questions we want GPT to answer\n",
    "   \n",
    "    questions = [\"I want to collect the paper has anaerobic biodegradation of specific chemical in a batch reactor using sludge or sediment as the inoculum.\\\n",
    "        The chemical should not be surfactant, polymer, plastic, filler, raw wastewater, or hospital wastewater. The batch reactor should not be a membrane, bed, column reactor, UASB, or having continuous influence or electric supply.\\\n",
    "        The sludge or sediment should be directly used in the batch reactor rather than bacteria, colony, strain, or consortium.\"]\n",
    "   \n",
    "    # Construct the prompt by combining the paper text and the questions\n",
    "    prompt = f\"Please analyze the following academic paper and answer the following questions first with 'yes' or 'no':\\n\\n{clean_text}\\n\\n and your reason for why it satisfies or not.\"\n",
    "    for i, question in enumerate(questions, start=1):\n",
    "        prompt += f\"{i}. {question}\\n\"\n",
    "    \n",
    "    # Send the prompt to GPT and get a response\n",
    "    response = client.chat.completions.create(\n",
    "        #model=\"gpt-3.5-turbo\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a research paper reviewer. Please use text and respond to the questions with 'yes' or 'no' and then reasons.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0 \n",
    "    )\n",
    "    \n",
    "    # Return the GPT response\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stage_1_resu = []\n",
    "for i in range (1,101):\n",
    "    file_path = str(i)+\"_clean_text.txt\"\n",
    "    print(i)\n",
    "    #print(file_path)\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        clean_text = file.read()\n",
    "        print(len(clean_text))\n",
    "        cleaned_text = clean_text.replace(\"Title: \", \"\").replace(\"Paragraph: \", \"\")\n",
    "        gpt_response = chat_gpt(cleaned_text)\n",
    "        print(gpt_response)\n",
    "        stage_1_resu.append(gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 question\n",
    "import pandas as pd\n",
    "\n",
    "# Extract the initial letter and full response into a structured list\n",
    "formatted_data = [[item[0], item] for item in stage_1_resu]\n",
    "\n",
    "# Create a DataFrame with the required structure\n",
    "df = pd.DataFrame(formatted_data, columns=[\"Initial\", \"Full Answer\"])\n",
    "\n",
    "print(df)\n",
    "df.to_excel('Stage_1_answer.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2 -- Extracting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 -- I. Summarizing Relevant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques_text = [\"Which chemicals' anaerobic biodegradation performance were studied? Use the full name of the chemicals. And how much of them were added?\", \n",
    " \"How did they build the biodegradation experiment batch reactors, list all the content, especially not bypass the numbers?\",\n",
    " \"How much inoculum (sludge or sediment) was added in the anaerobic biodegradation experiments?\",\n",
    " \"How much liquid medium was added? What is the name and initial pH of the medium used in the batch reactors?\",\n",
    " \"What temperature and pH was the anaerobic biodegradation experiments(or batch reactors) incubated at?\"]\n",
    " #,\"Double check your answer based on my question.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"...\",  # Replace with your actual OpenAI API key\n",
    ")\n",
    "\n",
    "def stageII_1st(clean_text,questions):\n",
    "    # Define the questions we want GPT to answer\n",
    "    questions = questions\n",
    "    \n",
    "    # Construct the prompt by combining the paper text and the questions\n",
    "    prompt = f\"Please analyze this academic paper and answer the following questions. Note if no related infomation you can say there is no related infomation. \\n{clean_text}\"\n",
    "    \n",
    "    for i, question in enumerate(questions, start=1):\n",
    "        prompt += f\"{i}. {question}\\n\"\n",
    "    \n",
    "    # Send the prompt to GPT and get a response\n",
    "    response = client.chat.completions.create(\n",
    "        #model=\"gpt-3.5-turbo\",\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a research paper reviewer. Please use the text and respond to the questions with all relevant information.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0 \n",
    "    )\n",
    "    \n",
    "    # Return the GPT response\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_all = []\n",
    "useful_txt_ind = [1]#,4,9,11]\n",
    "\n",
    "for i in useful_txt_ind:\n",
    "\n",
    "    file_path = \"C:...\"\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        clean_text = file.read()\n",
    "\n",
    "    cleaned_text = clean_text.replace(\"Title: \", \"\").replace(\"Paragraph: \", \"\")\n",
    "    \n",
    "    ans = chat_gpt(cleaned_text,ques_text)\n",
    "    answers_all.append(ans)\n",
    "    print (len(answers_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2 -- II. Extracting Target Features and Formatting as an Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import constants\n",
    "\n",
    "\n",
    "# Define the schema and required fields\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"chemical_names\": {\"type\": \"string\"},\n",
    "        \"chemical_amounts\": {\"type\": \"string\"},\n",
    "        \"inoculum_name\": {\"type\": \"string\"},\n",
    "        \"inoculum_dosage\": {\"type\": \"string\"},\n",
    "        \"medium_name\": {\"type\": \"string\"},\n",
    "        \"medium_volume\": {\"type\": \"string\"},\n",
    "        \"pHs\": {\"type\": \"string\"},\n",
    "        \"Temperatures\": {\"type\": \"integer\"}\n",
    "    },\n",
    "    \"required\": [\"chemical_names\", \"chemical_amounts\", \"inoculum_name\", \"inoculum_dosage\", \"medium_name\", \"medium_volume\", \"pHs\", \"Temperatures\"]\n",
    "}\n",
    "def stageII_2nd(gpt_response,schema):\n",
    "    schema = schema\n",
    "\n",
    "    # Prompt to guide GPT to extract information as per the schema\n",
    "    prompt = f\"\"\"\n",
    "    Given the following text, act as a data analyst and collect relevant information from the text. \n",
    "    Please extract the corresponding information (corresponding to each key in the schema) from the inputs. \n",
    "    Here is the description for each key:\n",
    "    chemical_names: what chemical is studied and try to list all chemicals studied for anaerobic biodegradation.\n",
    "    chemical_amounts: The chemical concentrations in the batch reactor.\n",
    "    inoculum_name: The inoculum name in batch reactor.\n",
    "    inoculum_dosage: the amount or concentration of the added inoculum in batch reactor.\n",
    "    medium_name: if just mentioned as medium, then just output medium.\n",
    "    medium_volume: the amount of the added liquid medium or the working volume.\n",
    "    pHs: the initial pH for the batch reactor.\n",
    "    Temperatures: the batch reactor is incubated at what temperature.\n",
    "    {schema}\n",
    "\n",
    "    Text:\n",
    "    {gpt_response}\n",
    "\n",
    "    Return the data in a structured format based on the schema.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use GPT-3.5 turbo to process the prompt\n",
    "    response = client.chat.completions.create(\n",
    "        #model=\"gpt-3.5-turbo\",\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a data extractor.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0  # You can set the temperature to 0 for more deterministic results\n",
    "    )\n",
    "\n",
    "    # Get the response\n",
    "    structured_output = response.choices[0].message.content.strip()\n",
    "    return structured_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "ind = pd.read_excel(...)\n",
    "answers_1st = []\n",
    "answers_2nd = []\n",
    "useful_txt_ind = ind\n",
    "\n",
    "for i in useful_txt_ind:\n",
    "\n",
    "    print (i)\n",
    "    file_path = \"C:...\"\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        clean_text = file.read()\n",
    "\n",
    "    cleaned_text = clean_text.replace(\"Title: \", \"\").replace(\"Paragraph: \", \"\")\n",
    "    \n",
    "    ans_1st = stageII_1st(cleaned_text,ques_text)\n",
    "    answers_1st.append(ans_1st)\n",
    "    ans_2nd = stageII_2nd(ans_1st,schema)\n",
    "    answers_2nd.append(ans_2nd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Parse each JSON string into a Python dictionary\n",
    "cleaned_parsed_data = [item.replace(\"```json\", \"\").replace(\"```\", \"\").strip() for item in answers_2nd]\n",
    "parsed_data = [json.loads(item) for item in cleaned_parsed_data]\n",
    "\n",
    "# Convert the list of dictionaries into a DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "df['DOI_ind'] = useful_txt_ind\n",
    "df['ans_1st'] = answers_1st\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('output....xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemrel_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
